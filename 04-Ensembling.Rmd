---
title: "04-Ensembling"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(tidymodels)
library(tune)

source(here::here("materials/exercises/04-helpers.R"))

fit_split <- function(formula, model, split, ...) {
  wf <- workflows::add_model(workflows::add_formula(workflows::workflow(), formula, blueprint = hardhat::default_formula_blueprint(indicators = FALSE)), model)
  tune::last_fit(wf, split, ...)
}

get_tree_fit <- function(results) {
  results %>% 
    pluck(".workflow", 1) %>% 
    workflows::pull_workflow_fit() 
}

# read in the data
stackoverflow <- read_rds(here::here("materials/data/stackoverflow.rds"))

# split the data
set.seed(100) # Important!
so_split <- initial_split(stackoverflow, strata = remote)
so_train <- training(so_split)
so_test  <- testing(so_split)
```

# Your turn 1

Fill in the blanks to return the accuracy and ROC AUC for the vanilla decision tree model.

```{r}
vanilla_tree_spec <-
  decision_tree() %>% 
  set_engine("rpart") %>%
  set_mode("classification")

set.seed(100) # Important!
fit_split(remote ~ ., 
          model = vanilla_tree_spec, 
          split = so_split) %>% 
  collect_metrics()
```


# Your Turn 2

Create a new classification tree model spec; call it `big_tree_spec`. 
Set the cost complexity to `0`, and the minimum number of data points in a node to split to be `1`. 

Compare the metrics of the big tree to the vanilla tree- which one predicts the test set better?

*Hint: you'll need https://tidymodels.github.io/parsnip/reference/decision_tree.html*

```{r}
big_tree_spec <-
  decision_tree() %>% 
  set_engine("rpart") %>%
  set_mode("classification") %>% 
  set_args(cost_complexity=0,min_n=1)

set.seed(100) # Important!
fit_split(remote ~ ., 
          model = big_tree_spec, 
          split = so_split) %>% 
  collect_metrics()

```


# Your Turn 3

Let's combine bootstrapping with decision trees.

Do **Round 1** on your handouts.

# Your Turn 4

Now, let's add the aggregating part.

Do **Round 2** on your handouts.

# Your Turn 5

Create a new model spec called `rf_spec`, which will learn an ensemble of classification trees from our training data using the **ranger** package. 

Compare the metrics of the random forest to your two single tree models (vanilla and big)- which predicts the test set better?

*Hint: you'll need https://tidymodels.github.io/parsnip/articles/articles/Models.html*

```{r}
rf_spec <-  rand_forest() %>% 
  set_engine("ranger") %>%
  set_mode("classification")

set.seed(100)
fit_split(remote ~ ., 
          model = rf_spec, 
          split = so_split) %>% 
  collect_metrics()

```

# Your Turn 6

Challenge: Make 4 more random forest model specs, each using 4, 8, 12, and 20 variables at each split. Which value maximizes the area under the ROC curve?

*Hint: you'll need https://tidymodels.github.io/parsnip/reference/rand_forest.html*

```{r}
rf_spec <-  rand_forest(mtry = 4) %>% 
  set_engine("ranger") %>%
  set_mode("classification")

set.seed(100)
fit_split(remote ~ ., 
          model = rf_spec, 
          split = so_split) %>% 
  collect_metrics()
```

```{r}
rf_spec <-  rand_forest(mtry = 8) %>% 
  set_engine("ranger") %>%
  set_mode("classification")

set.seed(100)
fit_split(remote ~ ., 
          model = rf_spec, 
          split = so_split) %>% 
  collect_metrics()
```

```{r}
rf_spec <-  rand_forest(mtry = 12) %>% 
  set_engine("ranger") %>%
  set_mode("classification")

set.seed(100)
fit_split(remote ~ ., 
          model = rf_spec, 
          split = so_split) %>% 
  collect_metrics()
```

```{r}
rf_spec <-  rand_forest(mtry = 20) %>% 
  set_engine("ranger") %>%
  set_mode("classification")

set.seed(100)
fit_split(remote ~ ., 
          model = rf_spec, 
          split = so_split) %>% 
  collect_metrics()
```
```{r}

mtrys <- c(1,2,4,6,8,15,20)

run_forest <- function(mtrys){
  rf_spec <-  rand_forest(mtry = mtrys) %>% 
    set_engine("ranger") %>%
    set_mode("classification")
  
  set.seed(100)
  metric <- fit_split(remote ~ ., 
            model = rf_spec, 
            split = so_split) %>% 
    collect_metrics()
  metric <- metric %>% mutate(mtry=mtrys)
}

map(mtrys,run_forest) %>% bind_rows() %>% 
  ggplot(aes(mtry,.estimate,color=.metric)) + geom_line()
```


# Your Turn 7

Make a new model spec called `treebag_imp_spec` to fit a bagged classification tree model. Set the variable `importance` mode to "permutation". Plot the variable importance- which variable was the most important?

```{r}
treebag_spec <-
  rand_forest(mtry = .preds()) %>%
  set_engine("ranger") %>% 
  set_mode("classification")
set.seed(100)
fit_split(remote ~ ., 
          model = treebag_spec,
          split = so_split) %>% 
  collect_metrics()
```


```{r}
treebag_imp_spec <-
  rand_forest(mtry = 4) %>%
  set_engine("ranger",importance = "impurity") %>% 
  set_mode("classification")
set.seed(100)

imp_fit <- fit_split(remote ~ ., 
          model = treebag_imp_spec,
          split = so_split)

imp_plot <-get_tree_fit(imp_fit)
vip::vip(imp_plot, geom = "point")
```


